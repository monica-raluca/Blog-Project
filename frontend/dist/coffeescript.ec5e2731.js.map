{"mappings":"A,C,E,A,A,W,iB,C,Q,A,E,Q,S,C,C,C,E,O,c,C,E,O,C,e,C,I,I,E,I,K,E,W,C,E,a,C,C,GCAA,IAAI,EAAa,QAEjB,SAAS,EAAW,CAAK,EACvB,OAAO,AAAI,OAAO,MAAQ,EAAM,IAAI,CAAC,OAAS,QAChD,CAEA,IAAI,EAAY,mHACZ,EAAa,gCACb,EAAc,4BACd,EAAS,6BAET,EAAgB,EAAW,CAAC,MAAO,KAAM,MACb,KAAM,OAAQ,KACd,aAAc,SAAS,EACnD,EAAiB,CAAC,MAAO,QAAS,OAAQ,KAAM,SAAU,OACxC,SAAU,MAAO,QAAS,UAAW,QAAQ,CAK/D,EAAW,EAAW,EAAe,MAAM,CAJ1B,CAAC,QAAS,KAAM,WAAY,WAAY,SACvC,KAAM,KAAM,KAAM,MAAO,SAAU,OACnC,OAAQ,IAAK,QAAS,OAAQ,QAAS,UAAU,GAIvE,EAAiB,EAAW,GAG5B,IAAI,EAAiB,sBACjB,EAAgB,cAEhB,EAAY,EADM,CAAC,WAAY,MAAO,YAAa,OAAQ,OAAQ,QAAS,KAAM,MAAO,MAAO,KAAK,EAIzG,SAAS,EAAU,CAAM,CAAE,CAAK,EAE9B,GAAI,EAAO,GAAG,GAAI,CACZ,AAAsB,OAAtB,EAAM,KAAK,CAAC,KAAK,EAAW,CAAA,EAAM,KAAK,CAAC,KAAK,CAAG,CAAA,CAApD,EACA,IAAI,EAAc,EAAM,KAAK,CAAC,MAAM,CACpC,GAAI,EAAO,QAAQ,GAAI,CACrB,IAAI,EAAa,EAAO,WAAW,UACnC,AAAI,EAAa,GAAe,AAAoB,UAApB,EAAM,KAAK,CAAC,IAAI,CACvC,SACE,EAAa,EACf,SAEF,IACT,CACM,EAAc,GAChB,EAAO,EAAQ,EAGrB,CACA,GAAI,EAAO,QAAQ,GACjB,OAAO,KAGT,IAAI,EAAK,EAAO,IAAI,GAGpB,GAAI,EAAO,KAAK,CAAC,QAEf,OADA,EAAO,SAAS,GACT,UAIT,GAAI,EAAO,KAAK,CAAC,OAEf,OADA,EAAM,QAAQ,CAAG,EACV,EAAM,QAAQ,CAAC,EAAQ,GAIhC,GAAI,AAAO,MAAP,EAEF,OADA,EAAO,SAAS,GACT,UAIT,GAAI,EAAO,KAAK,CAAC,aAAc,CAAA,GAAQ,CACrC,IAAI,EAAe,CAAA,EAYnB,GAVI,EAAO,KAAK,CAAC,+BACf,CAAA,EAAe,CAAA,CADjB,EAGI,EAAO,KAAK,CAAC,gBACf,CAAA,EAAe,CAAA,CADjB,EAGI,EAAO,KAAK,CAAC,aACf,CAAA,EAAe,CAAA,CADjB,EAII,EAKF,MAHI,AAAiB,KAAjB,EAAO,IAAI,IACb,EAAO,MAAM,CAAC,GAET,SAGT,IAAI,EAAa,CAAA,EAajB,GAXI,EAAO,KAAK,CAAC,oBACf,CAAA,EAAa,CAAA,CADf,EAII,EAAO,KAAK,CAAC,8BACf,CAAA,EAAa,CAAA,CADf,EAII,EAAO,KAAK,CAAC,mBACf,CAAA,EAAa,CAAA,CADf,EAGI,EACF,MAAO,QAEX,CAGA,GAAI,EAAO,KAAK,CAAC,GAEf,OADA,EAAM,QAAQ,CAAG,EAAa,EAAO,OAAO,GAAI,CAAA,EAAO,UAChD,EAAM,QAAQ,CAAC,EAAQ,GAGhC,GAAI,EAAO,KAAK,CAAC,GACf,GAAI,AAAoB,KAApB,EAAO,OAAO,IAAa,EAAO,KAAK,CAAC,QAAS,CAAA,GAEnD,OADA,EAAM,QAAQ,CAAG,EAAa,EAAO,OAAO,GAAI,CAAA,EAAM,kBAC/C,EAAM,QAAQ,CAAC,EAAQ,QAE9B,EAAO,MAAM,CAAC,UAOlB,AAAI,EAAO,KAAK,CAAC,IAAc,EAAO,KAAK,CAAC,GACnC,WAEL,EAAO,KAAK,CAAC,GACR,cAGL,EAAO,KAAK,CAAC,GACR,OAGL,EAAO,KAAK,CAAC,IAAW,EAAM,IAAI,EAAI,EAAO,KAAK,CAAC,GAC9C,WAGL,EAAO,KAAK,CAAC,GACR,UAGL,EAAO,KAAK,CAAC,GACR,YAIT,EAAO,IAAI,GACJ,EACT,CAEA,SAAS,EAAa,CAAS,CAAE,CAAU,CAAE,CAAQ,EACnD,OAAO,SAAS,CAAM,CAAE,CAAK,EAC3B,KAAO,CAAC,EAAO,GAAG,IAEhB,GADA,EAAO,QAAQ,CAAC,aACZ,EAAO,GAAG,CAAC,MAEb,CAAA,GADA,EAAO,IAAI,GACP,GAAc,EAAO,GAAG,GAC1B,OAAO,CADT,MAGK,GAAI,EAAO,KAAK,CAAC,GAEtB,OADA,EAAM,QAAQ,CAAG,EACV,EAEP,EAAO,GAAG,CAAC,UAMf,OAHI,GACF,CAAA,EAAM,QAAQ,CAAG,CADnB,EAGO,CACT,CACF,CAEA,SAAS,EAAY,CAAM,CAAE,CAAK,EAChC,KAAO,CAAC,EAAO,GAAG,IAAI,CAEpB,GADA,EAAO,QAAQ,CAAC,QACZ,EAAO,KAAK,CAAC,OAAQ,CACvB,EAAM,QAAQ,CAAG,EACjB,KACF,CACA,EAAO,QAAQ,CAAC,IAClB,CACA,MAAO,SACT,CAEA,SAAS,EAAO,CAAM,CAAE,CAAK,CAAE,EAAO,QAAQ,EAE5C,IAAK,IADD,EAAS,EAAG,EAAQ,CAAA,EAAO,EAAc,KACpC,EAAQ,EAAM,KAAK,CAAE,EAAO,EAAQ,EAAM,IAAI,CACrD,GAAI,AAAe,WAAf,EAAM,IAAI,EAAiB,AAAc,KAAd,EAAM,IAAI,CAAS,CAChD,EAAS,EAAM,MAAM,CAAG,EAAO,UAAU,CACzC,KACF,CAEE,AAAS,WAAT,GACF,EAAQ,KACR,EAAc,EAAO,MAAM,GAAK,EAAO,OAAO,GAAG,MAAM,EAC9C,EAAM,KAAK,CAAC,KAAK,EAC1B,CAAA,EAAM,KAAK,CAAC,KAAK,CAAG,CAAA,CADtB,EAGA,EAAM,KAAK,CAAG,CACZ,OAAQ,EACR,KAAM,EACN,KAAM,EAAM,KAAK,CACjB,MAAO,EACP,YAAa,CACf,CACF,CAEA,SAAS,EAAO,CAAM,CAAE,CAAK,EAC3B,GAAK,EAAM,KAAK,CAAC,IAAI,CACrB,GAAI,AAAqB,WAArB,EAAM,KAAK,CAAC,IAAI,CAkBlB,OADA,EAAM,KAAK,CAAG,EAAM,KAAK,CAAC,IAAI,CACvB,CAAA,MAlB0B,CAGjC,IAAK,IAFD,EAAU,EAAO,WAAW,GAC5B,EAAU,CAAA,EACL,EAAQ,EAAM,KAAK,CAAE,EAAO,EAAQ,EAAM,IAAI,CACrD,GAAI,IAAY,EAAM,MAAM,CAAE,CAC5B,EAAU,CAAA,EACV,KACF,CAEF,GAAI,CAAC,EACH,MAAO,CAAA,EAET,KAAO,EAAM,KAAK,CAAC,IAAI,EAAI,EAAM,KAAK,CAAC,MAAM,GAAK,GAChD,EAAM,KAAK,CAAG,EAAM,KAAK,CAAC,IAAI,CAEhC,MAAO,CAAA,CACT,CAIF,CA+CO,IAAM,EAAe,CAC1B,KAAM,eACN,WAAY,WACV,MAAO,CACL,SAAU,EACV,MAAO,CAAC,OAAQ,EAAG,KAAK,SAAU,KAAM,KAAM,MAAO,CAAA,CAAK,EAC1D,KAAM,CAAA,EACN,OAAQ,CACV,CACF,EAEA,MAAO,SAAS,CAAM,CAAE,CAAK,EAC3B,IAAI,EAAY,AAAsB,OAAtB,EAAM,KAAK,CAAC,KAAK,EAAa,EAAM,KAAK,AACrD,CAAA,GAAa,EAAO,GAAG,IAAI,CAAA,EAAU,KAAK,CAAG,CAAA,CAAjD,EAEA,IAAI,EAAQ,AA5DhB,SAAoB,CAAM,CAAE,CAAK,EAC/B,IAAI,EAAQ,EAAM,QAAQ,CAAC,EAAQ,GAC/B,EAAU,EAAO,OAAO,EAGxB,AAAY,CAAA,WAAZ,GACF,CAAA,EAAM,MAAM,CAAG,CAAA,CADjB,EAGM,CAAA,AAAA,CAAA,AAAY,OAAZ,GAAoB,AAAY,OAAZ,CAAY,GAAS,EAAO,GAAG,IAClD,AAAU,WAAV,CAAU,GACf,EAAO,EAAQ,GAEjB,IAAI,EAAkB,MAAM,OAAO,CAAC,GAYpC,GAXI,AAAoB,KAApB,GACF,EAAO,EAAQ,EAAO,MAAM,KAAK,CAAC,EAAiB,EAAgB,IAEjE,EAAe,IAAI,CAAC,IACtB,EAAO,EAAQ,GAEb,AAAW,QAAX,GACF,EAAO,EAAQ,GAIb,AAAU,WAAV,GACE,EAAO,EAAQ,GACjB,OAAO,EAIX,GAAI,AAAoB,KADxB,CAAA,EAAkB,MAAM,OAAO,CAAC,EAAhC,EAC4B,CAC1B,KAAO,AAAoB,UAApB,EAAM,KAAK,CAAC,IAAI,EAAgB,EAAM,KAAK,CAAC,IAAI,EACrD,EAAM,KAAK,CAAG,EAAM,KAAK,CAAC,IAAI,AAC5B,CAAA,EAAM,KAAK,CAAC,IAAI,EAAI,GACtB,CAAA,EAAM,KAAK,CAAG,EAAM,KAAK,CAAC,IAAI,AAAJ,CAC9B,CAOA,OANI,EAAM,MAAM,EAAI,EAAO,GAAG,KACxB,AAAoB,UAApB,EAAM,KAAK,CAAC,IAAI,EAAgB,EAAM,KAAK,CAAC,IAAI,EAClD,CAAA,EAAM,KAAK,CAAG,EAAM,KAAK,CAAC,IAAI,AAAJ,EAC5B,EAAM,MAAM,CAAG,CAAA,GAGV,AAAS,UAAT,GAAqB,AAAS,UAAT,EAAoB,KAAO,CACzD,EAiB2B,EAAQ,GAM/B,OALI,GAAS,AAAS,WAAT,IACP,GAAW,CAAA,EAAU,KAAK,CAAG,CAAA,CAAjC,EACA,EAAM,IAAI,CAAG,AAAS,eAAT,GAA0B,AAAoB,KAApB,EAAO,OAAO,IAGhD,CACT,EAEA,OAAQ,SAAS,CAAK,CAAE,CAAI,EAC1B,GAAI,EAAM,QAAQ,EAAI,EAAW,OAAO,EACxC,IAAI,EAAQ,EAAM,KAAK,CACnB,EAAS,GAAQ,MAAM,OAAO,CAAC,EAAK,MAAM,CAAC,IAAM,GACrD,GAAI,EAAQ,KAAO,AAAc,UAAd,EAAM,IAAI,EAAgB,EAAM,IAAI,EAAE,EAAQ,EAAM,IAAI,CAC3E,IAAI,EAAS,GAAU,EAAM,IAAI,GAAK,EAAK,MAAM,CAAC,UAClD,AAAI,EAAM,KAAK,CACN,EAAM,WAAW,GAAI,EAEpB,AAAA,CAAA,EAAS,EAAM,IAAI,CAAG,CAAA,EAAO,MAAM,AAC/C,EAEA,aAAc,CACZ,cAAe,CAAC,KAAM,GAAG,CAC3B,CACF,C","sources":["<anon>","node_modules/@codemirror/legacy-modes/mode/coffeescript.js"],"sourcesContent":["\nfunction $parcel$export(e, n, v, s) {\n  Object.defineProperty(e, n, {get: v, set: s, enumerable: true, configurable: true});\n}\n\n      var $parcel$global = globalThis;\n    var parcelRequire = $parcel$global[\"parcelRequire10c2\"];\nvar parcelRegister = parcelRequire.register;\nparcelRegister(\"1l8mw\", function(module, exports) {\n\n$parcel$export(module.exports, \"coffeeScript\", () => $0f9e3d30a98c79b8$export$8f1a590246f3cf60);\nvar $0f9e3d30a98c79b8$var$ERRORCLASS = \"error\";\nfunction $0f9e3d30a98c79b8$var$wordRegexp(words) {\n    return new RegExp(\"^((\" + words.join(\")|(\") + \"))\\\\b\");\n}\nvar $0f9e3d30a98c79b8$var$operators = /^(?:->|=>|\\+[+=]?|-[\\-=]?|\\*[\\*=]?|\\/[\\/=]?|[=!]=|<[><]?=?|>>?=?|%=?|&=?|\\|=?|\\^=?|\\~|!|\\?|(or|and|\\|\\||&&|\\?)=)/;\nvar $0f9e3d30a98c79b8$var$delimiters = /^(?:[()\\[\\]{},:`=;]|\\.\\.?\\.?)/;\nvar $0f9e3d30a98c79b8$var$identifiers = /^[_A-Za-z$][_A-Za-z$0-9]*/;\nvar $0f9e3d30a98c79b8$var$atProp = /^@[_A-Za-z$][_A-Za-z$0-9]*/;\nvar $0f9e3d30a98c79b8$var$wordOperators = $0f9e3d30a98c79b8$var$wordRegexp([\n    \"and\",\n    \"or\",\n    \"not\",\n    \"is\",\n    \"isnt\",\n    \"in\",\n    \"instanceof\",\n    \"typeof\"\n]);\nvar $0f9e3d30a98c79b8$var$indentKeywords = [\n    \"for\",\n    \"while\",\n    \"loop\",\n    \"if\",\n    \"unless\",\n    \"else\",\n    \"switch\",\n    \"try\",\n    \"catch\",\n    \"finally\",\n    \"class\"\n];\nvar $0f9e3d30a98c79b8$var$commonKeywords = [\n    \"break\",\n    \"by\",\n    \"continue\",\n    \"debugger\",\n    \"delete\",\n    \"do\",\n    \"in\",\n    \"of\",\n    \"new\",\n    \"return\",\n    \"then\",\n    \"this\",\n    \"@\",\n    \"throw\",\n    \"when\",\n    \"until\",\n    \"extends\"\n];\nvar $0f9e3d30a98c79b8$var$keywords = $0f9e3d30a98c79b8$var$wordRegexp($0f9e3d30a98c79b8$var$indentKeywords.concat($0f9e3d30a98c79b8$var$commonKeywords));\n$0f9e3d30a98c79b8$var$indentKeywords = $0f9e3d30a98c79b8$var$wordRegexp($0f9e3d30a98c79b8$var$indentKeywords);\nvar $0f9e3d30a98c79b8$var$stringPrefixes = /^('{3}|\\\"{3}|['\\\"])/;\nvar $0f9e3d30a98c79b8$var$regexPrefixes = /^(\\/{3}|\\/)/;\nvar $0f9e3d30a98c79b8$var$commonConstants = [\n    \"Infinity\",\n    \"NaN\",\n    \"undefined\",\n    \"null\",\n    \"true\",\n    \"false\",\n    \"on\",\n    \"off\",\n    \"yes\",\n    \"no\"\n];\nvar $0f9e3d30a98c79b8$var$constants = $0f9e3d30a98c79b8$var$wordRegexp($0f9e3d30a98c79b8$var$commonConstants);\n// Tokenizers\nfunction $0f9e3d30a98c79b8$var$tokenBase(stream, state) {\n    // Handle scope changes\n    if (stream.sol()) {\n        if (state.scope.align === null) state.scope.align = false;\n        var scopeOffset = state.scope.offset;\n        if (stream.eatSpace()) {\n            var lineOffset = stream.indentation();\n            if (lineOffset > scopeOffset && state.scope.type == \"coffee\") return \"indent\";\n            else if (lineOffset < scopeOffset) return \"dedent\";\n            return null;\n        } else if (scopeOffset > 0) $0f9e3d30a98c79b8$var$dedent(stream, state);\n    }\n    if (stream.eatSpace()) return null;\n    var ch = stream.peek();\n    // Handle docco title comment (single line)\n    if (stream.match(\"####\")) {\n        stream.skipToEnd();\n        return \"comment\";\n    }\n    // Handle multi line comments\n    if (stream.match(\"###\")) {\n        state.tokenize = $0f9e3d30a98c79b8$var$longComment;\n        return state.tokenize(stream, state);\n    }\n    // Single line comment\n    if (ch === \"#\") {\n        stream.skipToEnd();\n        return \"comment\";\n    }\n    // Handle number literals\n    if (stream.match(/^-?[0-9\\.]/, false)) {\n        var floatLiteral = false;\n        // Floats\n        if (stream.match(/^-?\\d*\\.\\d+(e[\\+\\-]?\\d+)?/i)) floatLiteral = true;\n        if (stream.match(/^-?\\d+\\.\\d*/)) floatLiteral = true;\n        if (stream.match(/^-?\\.\\d+/)) floatLiteral = true;\n        if (floatLiteral) {\n            // prevent from getting extra . on 1..\n            if (stream.peek() == \".\") stream.backUp(1);\n            return \"number\";\n        }\n        // Integers\n        var intLiteral = false;\n        // Hex\n        if (stream.match(/^-?0x[0-9a-f]+/i)) intLiteral = true;\n        // Decimal\n        if (stream.match(/^-?[1-9]\\d*(e[\\+\\-]?\\d+)?/)) intLiteral = true;\n        // Zero by itself with no other piece of number.\n        if (stream.match(/^-?0(?![\\dx])/i)) intLiteral = true;\n        if (intLiteral) return \"number\";\n    }\n    // Handle strings\n    if (stream.match($0f9e3d30a98c79b8$var$stringPrefixes)) {\n        state.tokenize = $0f9e3d30a98c79b8$var$tokenFactory(stream.current(), false, \"string\");\n        return state.tokenize(stream, state);\n    }\n    // Handle regex literals\n    if (stream.match($0f9e3d30a98c79b8$var$regexPrefixes)) {\n        if (stream.current() != \"/\" || stream.match(/^.*\\//, false)) {\n            state.tokenize = $0f9e3d30a98c79b8$var$tokenFactory(stream.current(), true, \"string.special\");\n            return state.tokenize(stream, state);\n        } else stream.backUp(1);\n    }\n    // Handle operators and delimiters\n    if (stream.match($0f9e3d30a98c79b8$var$operators) || stream.match($0f9e3d30a98c79b8$var$wordOperators)) return \"operator\";\n    if (stream.match($0f9e3d30a98c79b8$var$delimiters)) return \"punctuation\";\n    if (stream.match($0f9e3d30a98c79b8$var$constants)) return \"atom\";\n    if (stream.match($0f9e3d30a98c79b8$var$atProp) || state.prop && stream.match($0f9e3d30a98c79b8$var$identifiers)) return \"property\";\n    if (stream.match($0f9e3d30a98c79b8$var$keywords)) return \"keyword\";\n    if (stream.match($0f9e3d30a98c79b8$var$identifiers)) return \"variable\";\n    // Handle non-detected items\n    stream.next();\n    return $0f9e3d30a98c79b8$var$ERRORCLASS;\n}\nfunction $0f9e3d30a98c79b8$var$tokenFactory(delimiter, singleline, outclass) {\n    return function(stream, state) {\n        while(!stream.eol()){\n            stream.eatWhile(/[^'\"\\/\\\\]/);\n            if (stream.eat(\"\\\\\")) {\n                stream.next();\n                if (singleline && stream.eol()) return outclass;\n            } else if (stream.match(delimiter)) {\n                state.tokenize = $0f9e3d30a98c79b8$var$tokenBase;\n                return outclass;\n            } else stream.eat(/['\"\\/]/);\n        }\n        if (singleline) state.tokenize = $0f9e3d30a98c79b8$var$tokenBase;\n        return outclass;\n    };\n}\nfunction $0f9e3d30a98c79b8$var$longComment(stream, state) {\n    while(!stream.eol()){\n        stream.eatWhile(/[^#]/);\n        if (stream.match(\"###\")) {\n            state.tokenize = $0f9e3d30a98c79b8$var$tokenBase;\n            break;\n        }\n        stream.eatWhile(\"#\");\n    }\n    return \"comment\";\n}\nfunction $0f9e3d30a98c79b8$var$indent(stream, state, type = \"coffee\") {\n    var offset = 0, align = false, alignOffset = null;\n    for(var scope = state.scope; scope; scope = scope.prev)if (scope.type === \"coffee\" || scope.type == \"}\") {\n        offset = scope.offset + stream.indentUnit;\n        break;\n    }\n    if (type !== \"coffee\") {\n        align = null;\n        alignOffset = stream.column() + stream.current().length;\n    } else if (state.scope.align) state.scope.align = false;\n    state.scope = {\n        offset: offset,\n        type: type,\n        prev: state.scope,\n        align: align,\n        alignOffset: alignOffset\n    };\n}\nfunction $0f9e3d30a98c79b8$var$dedent(stream, state) {\n    if (!state.scope.prev) return;\n    if (state.scope.type === \"coffee\") {\n        var _indent = stream.indentation();\n        var matched = false;\n        for(var scope = state.scope; scope; scope = scope.prev)if (_indent === scope.offset) {\n            matched = true;\n            break;\n        }\n        if (!matched) return true;\n        while(state.scope.prev && state.scope.offset !== _indent)state.scope = state.scope.prev;\n        return false;\n    } else {\n        state.scope = state.scope.prev;\n        return false;\n    }\n}\nfunction $0f9e3d30a98c79b8$var$tokenLexer(stream, state) {\n    var style = state.tokenize(stream, state);\n    var current = stream.current();\n    // Handle scope changes.\n    if (current === \"return\") state.dedent = true;\n    if ((current === \"->\" || current === \"=>\") && stream.eol() || style === \"indent\") $0f9e3d30a98c79b8$var$indent(stream, state);\n    var delimiter_index = \"[({\".indexOf(current);\n    if (delimiter_index !== -1) $0f9e3d30a98c79b8$var$indent(stream, state, \"])}\".slice(delimiter_index, delimiter_index + 1));\n    if ($0f9e3d30a98c79b8$var$indentKeywords.exec(current)) $0f9e3d30a98c79b8$var$indent(stream, state);\n    if (current == \"then\") $0f9e3d30a98c79b8$var$dedent(stream, state);\n    if (style === \"dedent\") {\n        if ($0f9e3d30a98c79b8$var$dedent(stream, state)) return $0f9e3d30a98c79b8$var$ERRORCLASS;\n    }\n    delimiter_index = \"])}\".indexOf(current);\n    if (delimiter_index !== -1) {\n        while(state.scope.type == \"coffee\" && state.scope.prev)state.scope = state.scope.prev;\n        if (state.scope.type == current) state.scope = state.scope.prev;\n    }\n    if (state.dedent && stream.eol()) {\n        if (state.scope.type == \"coffee\" && state.scope.prev) state.scope = state.scope.prev;\n        state.dedent = false;\n    }\n    return style == \"indent\" || style == \"dedent\" ? null : style;\n}\nconst $0f9e3d30a98c79b8$export$8f1a590246f3cf60 = {\n    name: \"coffeescript\",\n    startState: function() {\n        return {\n            tokenize: $0f9e3d30a98c79b8$var$tokenBase,\n            scope: {\n                offset: 0,\n                type: \"coffee\",\n                prev: null,\n                align: false\n            },\n            prop: false,\n            dedent: 0\n        };\n    },\n    token: function(stream, state) {\n        var fillAlign = state.scope.align === null && state.scope;\n        if (fillAlign && stream.sol()) fillAlign.align = false;\n        var style = $0f9e3d30a98c79b8$var$tokenLexer(stream, state);\n        if (style && style != \"comment\") {\n            if (fillAlign) fillAlign.align = true;\n            state.prop = style == \"punctuation\" && stream.current() == \".\";\n        }\n        return style;\n    },\n    indent: function(state, text) {\n        if (state.tokenize != $0f9e3d30a98c79b8$var$tokenBase) return 0;\n        var scope = state.scope;\n        var closer = text && \"])}\".indexOf(text.charAt(0)) > -1;\n        if (closer) while(scope.type == \"coffee\" && scope.prev)scope = scope.prev;\n        var closes = closer && scope.type === text.charAt(0);\n        if (scope.align) return scope.alignOffset - (closes ? 1 : 0);\n        else return (closes ? scope.prev : scope).offset;\n    },\n    languageData: {\n        commentTokens: {\n            line: \"#\"\n        }\n    }\n};\n\n});\n\n\n//# sourceMappingURL=coffeescript.ec5e2731.js.map\n","var ERRORCLASS = \"error\";\n\nfunction wordRegexp(words) {\n  return new RegExp(\"^((\" + words.join(\")|(\") + \"))\\\\b\");\n}\n\nvar operators = /^(?:->|=>|\\+[+=]?|-[\\-=]?|\\*[\\*=]?|\\/[\\/=]?|[=!]=|<[><]?=?|>>?=?|%=?|&=?|\\|=?|\\^=?|\\~|!|\\?|(or|and|\\|\\||&&|\\?)=)/;\nvar delimiters = /^(?:[()\\[\\]{},:`=;]|\\.\\.?\\.?)/;\nvar identifiers = /^[_A-Za-z$][_A-Za-z$0-9]*/;\nvar atProp = /^@[_A-Za-z$][_A-Za-z$0-9]*/;\n\nvar wordOperators = wordRegexp([\"and\", \"or\", \"not\",\n                                \"is\", \"isnt\", \"in\",\n                                \"instanceof\", \"typeof\"]);\nvar indentKeywords = [\"for\", \"while\", \"loop\", \"if\", \"unless\", \"else\",\n                      \"switch\", \"try\", \"catch\", \"finally\", \"class\"];\nvar commonKeywords = [\"break\", \"by\", \"continue\", \"debugger\", \"delete\",\n                      \"do\", \"in\", \"of\", \"new\", \"return\", \"then\",\n                      \"this\", \"@\", \"throw\", \"when\", \"until\", \"extends\"];\n\nvar keywords = wordRegexp(indentKeywords.concat(commonKeywords));\n\nindentKeywords = wordRegexp(indentKeywords);\n\n\nvar stringPrefixes = /^('{3}|\\\"{3}|['\\\"])/;\nvar regexPrefixes = /^(\\/{3}|\\/)/;\nvar commonConstants = [\"Infinity\", \"NaN\", \"undefined\", \"null\", \"true\", \"false\", \"on\", \"off\", \"yes\", \"no\"];\nvar constants = wordRegexp(commonConstants);\n\n// Tokenizers\nfunction tokenBase(stream, state) {\n  // Handle scope changes\n  if (stream.sol()) {\n    if (state.scope.align === null) state.scope.align = false;\n    var scopeOffset = state.scope.offset;\n    if (stream.eatSpace()) {\n      var lineOffset = stream.indentation();\n      if (lineOffset > scopeOffset && state.scope.type == \"coffee\") {\n        return \"indent\";\n      } else if (lineOffset < scopeOffset) {\n        return \"dedent\";\n      }\n      return null;\n    } else {\n      if (scopeOffset > 0) {\n        dedent(stream, state);\n      }\n    }\n  }\n  if (stream.eatSpace()) {\n    return null;\n  }\n\n  var ch = stream.peek();\n\n  // Handle docco title comment (single line)\n  if (stream.match(\"####\")) {\n    stream.skipToEnd();\n    return \"comment\";\n  }\n\n  // Handle multi line comments\n  if (stream.match(\"###\")) {\n    state.tokenize = longComment;\n    return state.tokenize(stream, state);\n  }\n\n  // Single line comment\n  if (ch === \"#\") {\n    stream.skipToEnd();\n    return \"comment\";\n  }\n\n  // Handle number literals\n  if (stream.match(/^-?[0-9\\.]/, false)) {\n    var floatLiteral = false;\n    // Floats\n    if (stream.match(/^-?\\d*\\.\\d+(e[\\+\\-]?\\d+)?/i)) {\n      floatLiteral = true;\n    }\n    if (stream.match(/^-?\\d+\\.\\d*/)) {\n      floatLiteral = true;\n    }\n    if (stream.match(/^-?\\.\\d+/)) {\n      floatLiteral = true;\n    }\n\n    if (floatLiteral) {\n      // prevent from getting extra . on 1..\n      if (stream.peek() == \".\"){\n        stream.backUp(1);\n      }\n      return \"number\";\n    }\n    // Integers\n    var intLiteral = false;\n    // Hex\n    if (stream.match(/^-?0x[0-9a-f]+/i)) {\n      intLiteral = true;\n    }\n    // Decimal\n    if (stream.match(/^-?[1-9]\\d*(e[\\+\\-]?\\d+)?/)) {\n      intLiteral = true;\n    }\n    // Zero by itself with no other piece of number.\n    if (stream.match(/^-?0(?![\\dx])/i)) {\n      intLiteral = true;\n    }\n    if (intLiteral) {\n      return \"number\";\n    }\n  }\n\n  // Handle strings\n  if (stream.match(stringPrefixes)) {\n    state.tokenize = tokenFactory(stream.current(), false, \"string\");\n    return state.tokenize(stream, state);\n  }\n  // Handle regex literals\n  if (stream.match(regexPrefixes)) {\n    if (stream.current() != \"/\" || stream.match(/^.*\\//, false)) { // prevent highlight of division\n      state.tokenize = tokenFactory(stream.current(), true, \"string.special\");\n      return state.tokenize(stream, state);\n    } else {\n      stream.backUp(1);\n    }\n  }\n\n\n\n  // Handle operators and delimiters\n  if (stream.match(operators) || stream.match(wordOperators)) {\n    return \"operator\";\n  }\n  if (stream.match(delimiters)) {\n    return \"punctuation\";\n  }\n\n  if (stream.match(constants)) {\n    return \"atom\";\n  }\n\n  if (stream.match(atProp) || state.prop && stream.match(identifiers)) {\n    return \"property\";\n  }\n\n  if (stream.match(keywords)) {\n    return \"keyword\";\n  }\n\n  if (stream.match(identifiers)) {\n    return \"variable\";\n  }\n\n  // Handle non-detected items\n  stream.next();\n  return ERRORCLASS;\n}\n\nfunction tokenFactory(delimiter, singleline, outclass) {\n  return function(stream, state) {\n    while (!stream.eol()) {\n      stream.eatWhile(/[^'\"\\/\\\\]/);\n      if (stream.eat(\"\\\\\")) {\n        stream.next();\n        if (singleline && stream.eol()) {\n          return outclass;\n        }\n      } else if (stream.match(delimiter)) {\n        state.tokenize = tokenBase;\n        return outclass;\n      } else {\n        stream.eat(/['\"\\/]/);\n      }\n    }\n    if (singleline) {\n      state.tokenize = tokenBase;\n    }\n    return outclass;\n  };\n}\n\nfunction longComment(stream, state) {\n  while (!stream.eol()) {\n    stream.eatWhile(/[^#]/);\n    if (stream.match(\"###\")) {\n      state.tokenize = tokenBase;\n      break;\n    }\n    stream.eatWhile(\"#\");\n  }\n  return \"comment\";\n}\n\nfunction indent(stream, state, type = \"coffee\") {\n  var offset = 0, align = false, alignOffset = null;\n  for (var scope = state.scope; scope; scope = scope.prev) {\n    if (scope.type === \"coffee\" || scope.type == \"}\") {\n      offset = scope.offset + stream.indentUnit;\n      break;\n    }\n  }\n  if (type !== \"coffee\") {\n    align = null;\n    alignOffset = stream.column() + stream.current().length;\n  } else if (state.scope.align) {\n    state.scope.align = false;\n  }\n  state.scope = {\n    offset: offset,\n    type: type,\n    prev: state.scope,\n    align: align,\n    alignOffset: alignOffset\n  };\n}\n\nfunction dedent(stream, state) {\n  if (!state.scope.prev) return;\n  if (state.scope.type === \"coffee\") {\n    var _indent = stream.indentation();\n    var matched = false;\n    for (var scope = state.scope; scope; scope = scope.prev) {\n      if (_indent === scope.offset) {\n        matched = true;\n        break;\n      }\n    }\n    if (!matched) {\n      return true;\n    }\n    while (state.scope.prev && state.scope.offset !== _indent) {\n      state.scope = state.scope.prev;\n    }\n    return false;\n  } else {\n    state.scope = state.scope.prev;\n    return false;\n  }\n}\n\nfunction tokenLexer(stream, state) {\n  var style = state.tokenize(stream, state);\n  var current = stream.current();\n\n  // Handle scope changes.\n  if (current === \"return\") {\n    state.dedent = true;\n  }\n  if (((current === \"->\" || current === \"=>\") && stream.eol())\n      || style === \"indent\") {\n    indent(stream, state);\n  }\n  var delimiter_index = \"[({\".indexOf(current);\n  if (delimiter_index !== -1) {\n    indent(stream, state, \"])}\".slice(delimiter_index, delimiter_index+1));\n  }\n  if (indentKeywords.exec(current)){\n    indent(stream, state);\n  }\n  if (current == \"then\"){\n    dedent(stream, state);\n  }\n\n\n  if (style === \"dedent\") {\n    if (dedent(stream, state)) {\n      return ERRORCLASS;\n    }\n  }\n  delimiter_index = \"])}\".indexOf(current);\n  if (delimiter_index !== -1) {\n    while (state.scope.type == \"coffee\" && state.scope.prev)\n      state.scope = state.scope.prev;\n    if (state.scope.type == current)\n      state.scope = state.scope.prev;\n  }\n  if (state.dedent && stream.eol()) {\n    if (state.scope.type == \"coffee\" && state.scope.prev)\n      state.scope = state.scope.prev;\n    state.dedent = false;\n  }\n\n  return style == \"indent\" || style == \"dedent\" ? null : style;\n}\n\nexport const coffeeScript = {\n  name: \"coffeescript\",\n  startState: function() {\n    return {\n      tokenize: tokenBase,\n      scope: {offset: 0, type:\"coffee\", prev: null, align: false},\n      prop: false,\n      dedent: 0\n    };\n  },\n\n  token: function(stream, state) {\n    var fillAlign = state.scope.align === null && state.scope;\n    if (fillAlign && stream.sol()) fillAlign.align = false;\n\n    var style = tokenLexer(stream, state);\n    if (style && style != \"comment\") {\n      if (fillAlign) fillAlign.align = true;\n      state.prop = style == \"punctuation\" && stream.current() == \".\"\n    }\n\n    return style;\n  },\n\n  indent: function(state, text) {\n    if (state.tokenize != tokenBase) return 0;\n    var scope = state.scope;\n    var closer = text && \"])}\".indexOf(text.charAt(0)) > -1;\n    if (closer) while (scope.type == \"coffee\" && scope.prev) scope = scope.prev;\n    var closes = closer && scope.type === text.charAt(0);\n    if (scope.align)\n      return scope.alignOffset - (closes ? 1 : 0);\n    else\n      return (closes ? scope.prev : scope).offset;\n  },\n\n  languageData: {\n    commentTokens: {line: \"#\"}\n  }\n};\n"],"names":["parcelRequire","$parcel$global","globalThis","register","module","exports","Object","defineProperty","get","$0f9e3d30a98c79b8$export$8f1a590246f3cf60","set","s","enumerable","configurable","$0f9e3d30a98c79b8$var$ERRORCLASS","$0f9e3d30a98c79b8$var$wordRegexp","words","RegExp","join","$0f9e3d30a98c79b8$var$operators","$0f9e3d30a98c79b8$var$delimiters","$0f9e3d30a98c79b8$var$identifiers","$0f9e3d30a98c79b8$var$atProp","$0f9e3d30a98c79b8$var$wordOperators","$0f9e3d30a98c79b8$var$indentKeywords","$0f9e3d30a98c79b8$var$keywords","concat","$0f9e3d30a98c79b8$var$stringPrefixes","$0f9e3d30a98c79b8$var$regexPrefixes","$0f9e3d30a98c79b8$var$constants","$0f9e3d30a98c79b8$var$tokenBase","stream","state","sol","scope","align","scopeOffset","offset","eatSpace","lineOffset","indentation","type","$0f9e3d30a98c79b8$var$dedent","ch","peek","match","skipToEnd","tokenize","$0f9e3d30a98c79b8$var$longComment","floatLiteral","backUp","intLiteral","$0f9e3d30a98c79b8$var$tokenFactory","current","prop","next","delimiter","singleline","outclass","eol","eatWhile","eat","$0f9e3d30a98c79b8$var$indent","alignOffset","prev","indentUnit","column","length","_indent","matched","name","startState","dedent","token","fillAlign","style","$0f9e3d30a98c79b8$var$tokenLexer","delimiter_index","indexOf","slice","exec","indent","text","closer","charAt","closes","languageData","commentTokens","line"],"version":3,"file":"coffeescript.ec5e2731.js.map"}